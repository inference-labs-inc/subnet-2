{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TruthTorchLM — Quickstart: simple method tests and examples\n",
    "\n",
    "This minimal notebook shows how to:\n",
    "\n",
    "- Run a multiple‑LLM truthfulness check on 1–2 claims\n",
    "- Generate a short long‑form answer with an optional truth/score\n",
    "\n",
    "Prereqs:\n",
    "- Install the package in this kernel/env (uncomment if needed):\n",
    "  - `%pip install -U TruthTorchLM`\n",
    "  - or `%pip install -U git+https://github.com/Ybakman/TruthTorchLM`\n",
    "- Set at least one provider API key supported by your setup:\n",
    "  - `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, or `GOOGLE_API_KEY`\n"
   ],
   "id": "9090fca4ccc0fae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Quick environment diagnostics (helps ensure the correct venv/kernel)\n",
    "import sys, os, importlib.util\n",
    "print('Python:', sys.version)\n",
    "print('Kernel executable:', sys.executable)\n",
    "print('Has TruthTorchLM?', bool(importlib.util.find_spec('TruthTorchLM')))\n",
    "print('Has truthtorchlm?', bool(importlib.util.find_spec('truthtorchlm')))\n",
    "print('OPENAI_API_KEY set?', bool(os.getenv('OPENAI_API_KEY')))\n",
    "print('ANTHROPIC_API_KEY set?', bool(os.getenv('ANTHROPIC_API_KEY')))\n",
    "print('GOOGLE_API_KEY set?', bool(os.getenv('GOOGLE_API_KEY')))\n"
   ],
   "id": "88f3a3ea532456eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import package (simple)\n",
   "id": "50687417dbbe730d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import importlib, importlib.util, types\n",
    "\n",
    "def import_tt():\n",
    "    for name in ('TruthTorchLM', 'truthtorchlm'):\n",
    "        if importlib.util.find_spec(name) is not None:\n",
    "            mod = importlib.import_module(name)\n",
    "            return name, mod\n",
    "    raise ImportError(\n",
    "        'TruthTorchLM not found in this kernel.\\n'\n",
    "        'Install into THIS kernel or switch kernel, e.g.:\\n'\n",
    "        '  %pip install -U TruthTorchLM\\n'\n",
    "        'Or from GitHub:\\n'\n",
    "        '  %pip install -U git+https://github.com/Ybakman/TruthTorchLM\\n'\n",
    "        'Then restart the kernel.'\n",
    "    )\n",
    "\n",
    "PKG_NAME, ttlm = import_tt()\n",
    "version = getattr(ttlm, '__version__', None) or getattr(getattr(ttlm, 'version', None) or types.SimpleNamespace(), '__version__', None)\n",
    "print(f'Using package: {PKG_NAME}, version: {version}')\n"
   ],
   "id": "c3839aa15cd4069e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Provider keys check (network calls will be skipped if none present)\n",
   "id": "19af9b2f4527ca9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import textwrap, json\n",
    "CAN_CALL = any([\n",
    "    bool(os.getenv('OPENAI_API_KEY')),\n",
    "    bool(os.getenv('ANTHROPIC_API_KEY')),\n",
    "    bool(os.getenv('GOOGLE_API_KEY')),\n",
    "])\n",
    "if not CAN_CALL:\n",
    "    print(textwrap.dedent('''\n",
    "    No provider API keys detected — examples will be skipped.\n",
    "    Set one of: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY and re‑run.\n",
    "    '''))\n"
   ],
   "id": "cd28d7de65f26369"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helpers: find functions and extract scores\n",
   "id": "6e5e1588bbc1d390"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import inspect, re\n",
    "from typing import Callable, Iterable, Optional, Tuple, Any\n",
    "\n",
    "def find_callable(module, candidates: Iterable[str]) -> Tuple[Optional[str], Optional[Callable]]:\n",
    "    # Exact match only (keep it simple)\n",
    "    for name in candidates:\n",
    "        fn = getattr(module, name, None)\n",
    "        if callable(fn):\n",
    "            return name, fn\n",
    "    # If none exact, show a short hint and return None\n",
    "    looks = [a for a in dir(module) if re.search(r'(truth|long|gen|check|verify)', a, re.I)]\n",
    "    print('Could not find any of', list(candidates))\n",
    "    if looks:\n",
    "        print('Related names in the package:', looks[:12], '...')\n",
    "    return None, None\n",
    "\n",
    "# Try to pull a numeric truth score from various common shapes\n",
    "SCORE_KEYS = ('truth_score', 'truth', 'score', 'truthfulness')\n",
    "\n",
    "def extract_score(obj: Any) -> Optional[float]:\n",
    "    try:\n",
    "        # direct numeric\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "        # dict with score-ish fields\n",
    "        if isinstance(obj, dict):\n",
    "            for k in SCORE_KEYS:\n",
    "                if k in obj:\n",
    "                    try:\n",
    "                        return float(obj[k])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            # nested\n",
    "            for v in obj.values():\n",
    "                s = extract_score(v)\n",
    "                if isinstance(s, (int, float)):\n",
    "                    return float(s)\n",
    "        # list/tuple\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            for v in obj:\n",
    "                s = extract_score(v)\n",
    "                if isinstance(s, (int, float)):\n",
    "                    return float(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n"
   ],
   "id": "d3db1c2a84d06764"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Multiple‑LLM truthfulness check — simple run + smoke test\n",
   "id": "93238a45624fe164"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:12:10.414177Z",
     "start_time": "2025-10-29T05:12:10.335302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adjust names to match your installed version if needed\n",
    "multi_truth_candidates = (\n",
    "    'multiple_llm_truthfulness_check',\n",
    "    'multi_llm_truthfulness_check',\n",
    "    'multiple_llm_based_truthfulness_check',\n",
    ")\n",
    "name_mt, fn_mt = find_callable(ttlm, multi_truth_candidates)\n",
    "\n",
    "claims = [\n",
    "    'The Eiffel Tower is in Berlin.',            # false\n",
    "    'Water boils at 100°C at standard pressure.' # true\n",
    "]\n",
    "models = ['gpt-4o-mini']  # keep it to one lightweight model\n",
    "\n",
    "results_mt = []\n",
    "if fn_mt and CAN_CALL:\n",
    "    # Adapt to common signatures\n",
    "    kwargs_template = []\n",
    "    try:\n",
    "        sig = inspect.signature(fn_mt)\n",
    "    except Exception:\n",
    "        sig = None\n",
    "    for claim in claims:\n",
    "        kwargs = {}\n",
    "        if sig:\n",
    "            for pname in sig.parameters:\n",
    "                p = pname.lower()\n",
    "                if 'claim' in p or 'statement' in p or 'text' in p or 'prompt' in p or 'question' in p:\n",
    "                    kwargs[pname] = claim\n",
    "                elif 'llm' in p or 'model' in p:\n",
    "                    kwargs[pname] = models\n",
    "        try:\n",
    "            out = fn_mt(**kwargs) if kwargs else fn_mt(claim)\n",
    "            results_mt.append({'claim': claim, 'result': out})\n",
    "        except TypeError:\n",
    "            # Fallback: try positional with (claim, models)\n",
    "            try:\n",
    "                out = fn_mt(claim, models)\n",
    "                results_mt.append({'claim': claim, 'result': out})\n",
    "            except Exception as e:\n",
    "                print('Call failed for claim:', claim, '->', e)\n",
    "\n",
    "    # Simple smoke test if scores can be extracted\n",
    "    if len(results_mt) == 2:\n",
    "        s_false = extract_score(results_mt[0]['result'])\n",
    "        s_true  = extract_score(results_mt[1]['result'])\n",
    "        if s_false is not None and s_true is not None:\n",
    "            try:\n",
    "                assert s_true >= s_false, f'Expected true-claim score >= false-claim score, got {s_true} vs {s_false}'\n",
    "                print('Truthfulness score test passed:', s_true, '>=', s_false)\n",
    "            except AssertionError as ae:\n",
    "                print('Truthfulness score test FAILED:', ae)\n",
    "        else:\n",
    "            print('No numeric truth scores found — skipping score assertion.')\n",
    "else:\n",
    "    if not fn_mt:\n",
    "        print('Skipping — multi‑LLM truth function not found.')\n",
    "    elif not CAN_CALL:\n",
    "        print('Skipping — missing provider API keys.')\n"
   ],
   "id": "343edfa6d9a97f5c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_callable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Adjust names to match your installed version if needed\u001B[39;00m\n\u001B[32m      2\u001B[39m multi_truth_candidates = (\n\u001B[32m      3\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mmultiple_llm_truthfulness_check\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      4\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mmulti_llm_truthfulness_check\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      5\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mmultiple_llm_based_truthfulness_check\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      6\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m name_mt, fn_mt = \u001B[43mfind_callable\u001B[49m(ttlm, multi_truth_candidates)\n\u001B[32m      9\u001B[39m claims = [\n\u001B[32m     10\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mThe Eiffel Tower is in Berlin.\u001B[39m\u001B[33m'\u001B[39m,            \u001B[38;5;66;03m# false\u001B[39;00m\n\u001B[32m     11\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mWater boils at 100°C at standard pressure.\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;66;03m# true\u001B[39;00m\n\u001B[32m     12\u001B[39m ]\n\u001B[32m     13\u001B[39m models = [\u001B[33m'\u001B[39m\u001B[33mgpt-4o-mini\u001B[39m\u001B[33m'\u001B[39m]  \u001B[38;5;66;03m# keep it to one lightweight model\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'find_callable' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Long‑form generation with truth value — simple run + smoke test\n",
   "id": "13dec79931173797"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "long_form_candidates = (\n",
    "    'long_form_generation_with_truth_value',\n",
    "    'long_form_generation_with_truthfulness',\n",
    "    'generate_long_form_with_truth',\n",
    ")\n",
    "name_lf, fn_lf = find_callable(ttlm, long_form_candidates)\n",
    "\n",
    "prompt = 'Explain the history of the Eiffel Tower in ~120–200 words with factual references.'\n",
    "long_form_out = None\n",
    "if fn_lf and CAN_CALL:\n",
    "    try:\n",
    "        sig = inspect.signature(fn_lf)\n",
    "    except Exception:\n",
    "        sig = None\n",
    "    kwargs = {}\n",
    "    if sig:\n",
    "        for pname in sig.parameters:\n",
    "            p = pname.lower()\n",
    "            if 'prompt' in p or 'question' in p or 'topic' in p or 'query' in p or 'instruction' in p or 'text' in p:\n",
    "                kwargs[pname] = prompt\n",
    "            elif 'llm' in p or 'model' in p:\n",
    "                kwargs[pname] = 'gpt-4o-mini'\n",
    "            elif 'max_tokens' in p or 'length' in p or 'words' in p:\n",
    "                kwargs[pname] = 300\n",
    "            elif 'return' in p and 'score' in p:\n",
    "                kwargs[pname] = True\n",
    "    try:\n",
    "        long_form_out = fn_lf(**kwargs) if kwargs else fn_lf(prompt)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            long_form_out = fn_lf(prompt, 'gpt-4o-mini')\n",
    "        except Exception as e:\n",
    "            print('Long‑form call failed ->', e)\n",
    "\n",
    "    # Smoke tests: presence of text and (optional) score\n",
    "    text_val = None\n",
    "    if isinstance(long_form_out, str):\n",
    "        text_val = long_form_out\n",
    "    elif isinstance(long_form_out, dict):\n",
    "        for k in ('text', 'output', 'answer', 'content'):\n",
    "            if k in long_form_out and isinstance(long_form_out[k], str):\n",
    "                text_val = long_form_out[k]\n",
    "                break\n",
    "    if text_val:\n",
    "        try:\n",
    "            assert len(text_val) >= 80, f'Expected at least ~80 chars of text, got {len(text_val)}'\n",
    "            print('Long‑form text length test passed:', len(text_val), 'chars')\n",
    "        except AssertionError as ae:\n",
    "            print('Long‑form text length test FAILED:', ae)\n",
    "    else:\n",
    "        print('Could not locate generated text — skipping length assertion.')\n",
    "\n",
    "    s = extract_score(long_form_out)\n",
    "    if s is not None:\n",
    "        try:\n",
    "            # Soft bound check — just ensure it is a finite number\n",
    "            assert s == s and abs(s) < 1e6, f'Unreasonable score value: {s}'\n",
    "            print('Long‑form score looks reasonable:', s)\n",
    "        except AssertionError as ae:\n",
    "            print('Long‑form score test FAILED:', ae)\n",
    "else:\n",
    "    if not fn_lf:\n",
    "        print('Skipping — long‑form function not found.')\n",
    "    elif not CAN_CALL:\n",
    "        print('Skipping — missing provider API keys.')\n"
   ],
   "id": "3e4b3d93a4d178e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Done\n",
    "- You now have minimal, self‑contained examples for both features.\n",
    "- If a function name differs in your installed version, replace the candidate lists at the top of each section with the correct name and re‑run.\n"
   ],
   "id": "abd9bf74d6ad7027"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
