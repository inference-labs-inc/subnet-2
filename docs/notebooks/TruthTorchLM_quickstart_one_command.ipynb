{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:03:15.847963Z",
     "start_time": "2025-11-03T16:03:07.762305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Oneâ€‘command quickstart (Option B: local gguf via llama.cpp) for TruthTorchLM truthfulness check\n",
    "# - Runs fully offline using two small local models downloaded from Hugging Face.\n",
    "# - Edit CLAIM below and run this single cell.\n",
    "# - First run will download models and may take a few minutes; subsequent runs are cached.\n",
    "\n",
    "import sys, subprocess, importlib, importlib.util, inspect\n",
    "\n",
    "# --- lightweight installer ----------------------------------------------------\n",
    "def _ensure(pkg_import: str, pip_name: str | None = None):\n",
    "    try:\n",
    "        __import__(pkg_import)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name or pkg_import])\n",
    "\n",
    "# Ensure local LLM deps\n",
    "_ensure('llama_cpp', 'llama-cpp-python')\n",
    "_ensure('huggingface_hub')\n",
    "# Ensure Transformers stack for generate_with_truth_value path\n",
    "_ensure('transformers')\n",
    "_ensure('torch')\n",
    "_ensure('sentencepiece')\n",
    "_ensure('accelerate')\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# --- pick and download two small GGUF models (<10 GB each) --------------------\n",
    "# We try a few common repos/filenames and use the first that works for each.\n",
    "A_CANDIDATES = [\n",
    "    ('TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF', 'tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'),\n",
    "    ('TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF', 'tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf'),\n",
    "]\n",
    "B_CANDIDATES = [\n",
    "    ('Qwen/Qwen2.5-3B-Instruct-GGUF', 'qwen2.5-3b-instruct-q4_k_m.gguf'),\n",
    "    ('Qwen/Qwen2.5-3B-Instruct-GGUF', 'qwen2.5-3b-instruct-q5_k_m.gguf'),\n",
    "    ('TheBloke/phi-2-GGUF', 'phi-2.Q4_K_M.gguf'),  # fallback if Qwen GGUF variant not available\n",
    "]\n",
    "\n",
    "def _first_available(cands):\n",
    "    for repo_id, filename in cands:\n",
    "        try:\n",
    "            path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "            print(f\"Using model: {repo_id} :: {filename}\")\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Could not download any of the candidate models. Last error: {last_err}\")\n",
    "\n",
    "path_a = _first_available(A_CANDIDATES)\n",
    "try:\n",
    "    path_b = _first_available(B_CANDIDATES)\n",
    "except Exception:\n",
    "    print('Second model unavailable; using the first model for both slots (still runs, less diversity).')\n",
    "    path_b = path_a\n",
    "\n",
    "# --- minimal llama.cpp wrapper ------------------------------------------------\n",
    "_LLAMS = {}\n",
    "\n",
    "def _get_llama(path: str):\n",
    "    llm = _LLAMS.get(path)\n",
    "    if llm is None:\n",
    "        # n_ctx can be tuned; n_gpu_layers>0 offloads on Apple Silicon builds\n",
    "        llm = Llama(model_path=path, n_ctx=4096, n_gpu_layers=0, logits_all=False, verbose=False)\n",
    "        _LLAMS[path] = llm\n",
    "    return llm\n",
    "\n",
    "\n",
    "def _gen_llama(prompt: str, path: str, max_new_tokens: int = 256, temperature: float = 0.1) -> str:\n",
    "    llm = _get_llama(path)\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        stop=[\"</s>\", \"###\"],\n",
    "    )\n",
    "    return out['choices'][0]['text'].strip()\n",
    "\n",
    "# Two generator callables for the \"multi-LLM\" check\n",
    "gen_a = lambda prompt: _gen_llama(prompt, path_a)\n",
    "gen_b = lambda prompt: _gen_llama(prompt, path_b)\n",
    "\n",
    "# --- pick a small Transformers model for generate_with_truth_value ------------\n",
    "TF_MODEL_CANDIDATES = [\n",
    "    'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "    'Qwen/Qwen2.5-0.5B-Instruct',\n",
    "]\n",
    "TF_MODEL_ID = None\n",
    "TF_TOKENIZER = None\n",
    "TF_MODEL = None\n",
    "try:\n",
    "    from transformers import AutoTokenizer as _AutoTokenizer, AutoModelForCausalLM as _AutoModel\n",
    "    import torch as _torch\n",
    "    for _mid in TF_MODEL_CANDIDATES:\n",
    "        try:\n",
    "            TF_TOKENIZER = _AutoTokenizer.from_pretrained(_mid, use_fast=True)\n",
    "            TF_MODEL_ID = _mid\n",
    "            # Try to load a small model locally; if it fails, we will pass the model id string instead\n",
    "            try:\n",
    "                _dtype = _torch.float16 if (_torch.cuda.is_available() or (_torch.backends.mps.is_available() if hasattr(_torch.backends, 'mps') else False)) else _torch.float32\n",
    "                TF_MODEL = _AutoModel.from_pretrained(TF_MODEL_ID, dtype=_dtype, low_cpu_mem_usage=True)\n",
    "                print(f'Using Transformers model (loaded): {TF_MODEL_ID}')\n",
    "            except Exception:\n",
    "                TF_MODEL = None\n",
    "                print(f'Using Transformers model id (lazy load in library): {TF_MODEL_ID}')\n",
    "            break\n",
    "        except Exception:\n",
    "            TF_TOKENIZER = None\n",
    "            continue\n",
    "except Exception:\n",
    "    TF_MODEL_ID = None\n",
    "    TF_TOKENIZER = None\n",
    "    TF_MODEL = None\n",
    "\n",
    "# --- TruthTorchLM integration --------------------------------------------------\n",
    "# Import TruthTorchLM module\n",
    "modname = 'TruthTorchLM' if importlib.util.find_spec('TruthTorchLM') else 'truthtorchlm'\n",
    "ttlm = importlib.import_module(modname)\n",
    "\n",
    "# Claim to evaluate (edit as you wish)\n",
    "CLAIM = 'The capital city of Washington State is Seattle.'\n",
    "\n",
    "# Ensure TruthTorchLM uses our local generator internally if it calls `generation(...)`\n",
    "if hasattr(ttlm, 'generation') and callable(getattr(ttlm, 'generation')):\n",
    "    def _generation_local(prompt: str, *args, **kwargs):\n",
    "        # Route all internal generation to our first local model\n",
    "        return gen_a(prompt)\n",
    "    ttlm.generation = _generation_local\n",
    "\n",
    "# Locate a single-claim truth evaluation function first\n",
    "# Prefer generator-based single-call functions for offline/local use\n",
    "GEN_TRUTH_CANDIDATES = [\n",
    "    'generate_with_truth_value',\n",
    "    'generate_with_truthfulness',\n",
    "    'long_form_generation_with_truth_value',\n",
    "]\n",
    "EVAL_CANDIDATES = [\n",
    "    'evaluate_truth_method',\n",
    "    'evaluate_truthfulness',\n",
    "    'evaluate_truth',\n",
    "    'truth_evaluate',\n",
    "]\n",
    "\n",
    "eval_fn = None\n",
    "used_name = None\n",
    "# Try generator-style first\n",
    "for n in GEN_TRUTH_CANDIDATES:\n",
    "    fn = getattr(ttlm, n, None)\n",
    "    if callable(fn):\n",
    "        eval_fn = fn\n",
    "        used_name = n\n",
    "        break\n",
    "# If not found, fall back to evaluate_* APIs that may need dataset/model\n",
    "if eval_fn is None:\n",
    "    for n in EVAL_CANDIDATES:\n",
    "        fn = getattr(ttlm, n, None)\n",
    "        if callable(fn):\n",
    "            eval_fn = fn\n",
    "            used_name = n\n",
    "            break\n",
    "\n",
    "if eval_fn is None:\n",
    "    # As a last resort, show nearby names and exit gracefully\n",
    "    near = [a for a in dir(ttlm) if any(k in a.lower() for k in ['truth', 'check', 'verify', 'generate'])]\n",
    "    print('Could not locate a single-claim truth evaluation function in TruthTorchLM.')\n",
    "    print('Package exposes similar attributes:', near)\n",
    "else:\n",
    "    # Build kwargs adaptively based on the function signature\n",
    "    kwargs = {}\n",
    "    try:\n",
    "        sig = inspect.signature(eval_fn)\n",
    "    except Exception:\n",
    "        sig = None\n",
    "    print(f'Using function: {used_name} with signature: {sig}')\n",
    "\n",
    "    # Select valid truth method objects (not booleans or enum names)\n",
    "    def _select_truth_methods(mod):\n",
    "        tms = []\n",
    "        tm_attr = getattr(mod, 'truth_methods', None)\n",
    "        if isinstance(tm_attr, dict):\n",
    "            tms = list(tm_attr.values())\n",
    "        elif isinstance(tm_attr, (list, tuple)):\n",
    "            tms = list(tm_attr)\n",
    "        # Keep only objects that expose REQUIRES_* flags expected by the library\n",
    "        def _is_valid(x):\n",
    "            for attr in ('REQUIRES_SAMPLED_TEXT','REQUIRES_SAMPLED_LOGITS','REQUIRES_SAMPLED_LOGPROBS','REQUIRES_SAMPLED_ATTENTIONS','REQUIRES_SAMPLED_ACTIVATIONS'):\n",
    "                if hasattr(x, attr):\n",
    "                    return True\n",
    "            return False\n",
    "        valid = [x for x in tms if _is_valid(x)]\n",
    "        if not valid:\n",
    "            print('Warning: could not locate valid truth method objects in ttlm.truth_methods; proceeding with none.')\n",
    "        return valid[:2]\n",
    "\n",
    "    if sig:\n",
    "        # Prepare common helper values\n",
    "        tm_list = _select_truth_methods(ttlm)\n",
    "        sys_prompt = getattr(ttlm, 'GOOGLE_CHECK_QUERY_SYSTEM_PROMPT', None)\n",
    "        # Build a simple 2-message chat compatible with many truth scorers\n",
    "        msgs = [\n",
    "            {'role': 'system', 'content': sys_prompt or 'You are a helpful, truthful assistant.'},\n",
    "            {'role': 'user', 'content': f'Determine if the following claim is true or false and explain briefly: {CLAIM}'},\n",
    "        ]\n",
    "        for pname in sig.parameters:\n",
    "            p = pname.lower()\n",
    "            # Single-claim direct argument\n",
    "            if any(k in p for k in ['claim', 'statement']):\n",
    "                kwargs[pname] = CLAIM\n",
    "            # Question field commonly used by generate_with_truth_value\n",
    "            elif 'question' in p:\n",
    "                kwargs[pname] = CLAIM\n",
    "            # Required chat messages for generate_with_truth_value\n",
    "            elif 'messages' == p:\n",
    "                kwargs[pname] = msgs\n",
    "            # Dataset-like argument (list of claims)\n",
    "            elif any(k in p for k in ['dataset', 'data', 'claims', 'questions', 'texts', 'samples']):\n",
    "                kwargs[pname] = [CLAIM]\n",
    "            # Method selectors\n",
    "            elif ('truth_methods' in p or 'methods' == p):\n",
    "                if tm_list:\n",
    "                    kwargs[pname] = tm_list\n",
    "            elif 'method' in p and method_value is not None:\n",
    "                kwargs[pname] = method_value\n",
    "            # Tokenizer can be omitted for string model; leave None\n",
    "            elif 'tokenizer' in p:\n",
    "                # Prefer the loaded tokenizer; else leave None to let the library resolve\n",
    "                kwargs[pname] = TF_TOKENIZER if 'TF_TOKENIZER' in globals() else None\n",
    "            # Model/generator hooks\n",
    "            elif any(k in p for k in ['generator', 'callable']):\n",
    "                kwargs[pname] = gen_a\n",
    "            elif 'model' in p or 'llm' in p:\n",
    "                # For generator-style function, pass HF model object if loaded, else model id\n",
    "                if used_name in GEN_TRUTH_CANDIDATES:\n",
    "                    kwargs[pname] = TF_MODEL if (\"TF_MODEL\" in globals() and TF_MODEL is not None) else TF_MODEL_ID\n",
    "                else:\n",
    "                    kwargs[pname] = gen_a\n",
    "            # Avoid passing provider model strings since we are offline; rely on monkey-patched generation\n",
    "\n",
    "    # Helper to normalize the result\n",
    "    def _extract_score(obj):\n",
    "        try:\n",
    "            if isinstance(obj, (int, float)):\n",
    "                return float(obj)\n",
    "            if isinstance(obj, dict):\n",
    "                for k in ['truth_score', 'truth', 'score', 'truthfulness', 'truth_value']:\n",
    "                    if k in obj:\n",
    "                        try:\n",
    "                            return float(obj[k])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                for v in obj.values():\n",
    "                    s = _extract_score(v)\n",
    "                    if isinstance(s, (int, float)):\n",
    "                        return float(s)\n",
    "            if isinstance(obj, (list, tuple)):\n",
    "                for v in obj:\n",
    "                    s = _extract_score(v)\n",
    "                    if isinstance(s, (int, float)):\n",
    "                        return float(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    # Execute\n",
    "    used_name_effective = used_name\n",
    "    result = None\n",
    "    call_error = None\n",
    "    if used_name == 'evaluate_truth_method':\n",
    "        patterns = []\n",
    "        # Build dataset candidates\n",
    "        ds1 = [CLAIM]\n",
    "        ds2 = [{'claim': CLAIM}]\n",
    "        ds3 = [{'question': CLAIM}]\n",
    "        ds4 = [{'text': CLAIM}]\n",
    "        tm_list = [method_value] if method_value is not None else []\n",
    "        # Try different shapes for truth_methods\n",
    "        tm_obj = getattr(ttlm, 'truth_methods', None)\n",
    "        tm_variants = []\n",
    "        if tm_list:\n",
    "            tm_variants.append(('truth_methods', tm_list))\n",
    "        if tm_obj is not None:\n",
    "            tm_variants.append(('truth_methods', tm_obj))\n",
    "        # Different model forms: string label, callable, and None\n",
    "        model_variants = [\n",
    "            ('model', 'local-gguf'),\n",
    "            ('model', gen_a),\n",
    "            ('model', None),\n",
    "        ]\n",
    "        # Assemble combinations\n",
    "        datasets = [('dataset', ds) for ds in (ds1, ds2, ds3, ds4)]\n",
    "        for ds_kv in datasets:\n",
    "            for tm_kv in tm_variants or [('truth_methods', tm_list)]:\n",
    "                for mdl_kv in model_variants:\n",
    "                    kw = dict([ds_kv, tm_kv, mdl_kv])\n",
    "                    patterns.append(kw)\n",
    "        # Try calling with the built patterns\n",
    "        for kw in patterns:\n",
    "            try:\n",
    "                result = eval_fn(**kw)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                call_error = e\n",
    "                continue\n",
    "    if result is None:\n",
    "        try:\n",
    "            result = eval_fn(**kwargs) if kwargs else eval_fn(CLAIM)\n",
    "        except TypeError:\n",
    "            # Retry with minimal positional input\n",
    "            try:\n",
    "                result = eval_fn(CLAIM)\n",
    "            except Exception:\n",
    "                if used_name in GEN_TRUTH_CANDIDATES:\n",
    "                    # No further fallback\n",
    "                    raise\n",
    "                # Try a generator truth function if available\n",
    "                for n in GEN_TRUTH_CANDIDATES:\n",
    "                    fn = getattr(ttlm, n, None)\n",
    "                    if callable(fn):\n",
    "                        used_name_effective = n\n",
    "                        try:\n",
    "                            # Build minimal kwargs for the generator function\n",
    "                            sig2 = None\n",
    "                            try:\n",
    "                                sig2 = inspect.signature(fn)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            kwargs2 = {}\n",
    "                            if sig2:\n",
    "                                for pname in sig2.parameters:\n",
    "                                    p = pname.lower()\n",
    "                                    if any(k in p for k in ['prompt','question','text','instruction','claim','statement']):\n",
    "                                        kwargs2[pname] = CLAIM\n",
    "                                    elif any(k in p for k in ['generator','callable']):\n",
    "                                        kwargs2[pname] = gen_a\n",
    "                            result = fn(**kwargs2) if kwargs2 else fn(CLAIM)\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "    # Normalize and print\n",
    "    score = _extract_score(result)\n",
    "    label = None\n",
    "    if isinstance(score, (int, float)):\n",
    "        # Heuristic mapping: >=0.5 => true, <0.5 => false\n",
    "        label = 'true' if score >= 0.5 else 'false'\n",
    "    print(f'Used function: {used_name_effective}')\n",
    "    print('Claim:', CLAIM)\n",
    "    if score is not None:\n",
    "        print('Truth score:', score, f'-> label: {label}')\n",
    "    else:\n",
    "        print('Truth score: (not found in output)')\n",
    "    print('Raw result:')\n",
    "    print(result)\n",
    "\n",
    "    # Soft smoke check for this specific claim (expecting false)\n",
    "    if score is not None:\n",
    "        try:\n",
    "            assert score < 0.5, f'Expected a false-ish score (<0.5) for this false claim, got {score}'\n",
    "            print('Smoke test passed: score indicates false as expected.')\n",
    "        except AssertionError as ae:\n",
    "            print('Smoke test warning:', ae)\n"
   ],
   "id": "90a3f5b1014c2a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF :: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
      "Using model: Qwen/Qwen2.5-3B-Instruct-GGUF :: qwen2.5-3b-instruct-q4_k_m.gguf\n",
      "Using Transformers model (loaded): TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Using function: generate_with_truth_value with signature: (model: Union[transformers.modeling_utils.PreTrainedModel, str], messages: list, question: str = None, truth_methods: list = [], tokenizer: Union[transformers.tokenization_utils.PreTrainedTokenizer, transformers.tokenization_utils_fast.PreTrainedTokenizerFast] = None, generation_seed=None, batch_generation=True, add_generation_prompt=True, continue_final_message=False, context: str = '', **kwargs) -> dict\n",
      "Warning: could not locate valid truth method objects in ttlm.truth_methods; proceeding with none.\n",
      "Used function: generate_with_truth_value\n",
      "Claim: The capital city of Washington State is Seattle.\n",
      "Truth score: (not found in output)\n",
      "Raw result:\n",
      "{'generated_text': 'The claim \"The capital city of Washington State is Seattle\" is true. Seattle is the capital city of Washington State.', 'normalized_truth_values': [], 'unnormalized_truth_values': [], 'method_specific_outputs': [], 'all_ids': tensor([[    1,   529, 29989,  5205, 29989, 29958,    13,  3492,   526,   263,\n",
      "         27592, 20255, 29889,     2, 29871,    13, 29966, 29989,  1792, 29989,\n",
      "         29958,    13,  6362,   837,   457,   565,   278,  1494,  5995,   338,\n",
      "          1565,   470,  2089,   322,  5649, 23359, 29901,   450,  7483,  4272,\n",
      "           310,  7660,  4306,   338, 27689, 29889,     2, 29871,    13, 29966,\n",
      "         29989,   465, 22137, 29989, 29958,    13,  1576,  5995,   376,  1576,\n",
      "          7483,  4272,   310,  7660,  4306,   338, 27689, 29908,   338,  1565,\n",
      "         29889, 27689,   338,   278,  7483,  4272,   310,  7660,  4306, 29889,\n",
      "             2]]), 'generated_tokens': tensor([ 1576,  5995,   376,  1576,  7483,  4272,   310,  7660,  4306,   338,\n",
      "        27689, 29908,   338,  1565, 29889, 27689,   338,   278,  7483,  4272,\n",
      "          310,  7660,  4306, 29889,     2])}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fd85d3beee7f5438",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
